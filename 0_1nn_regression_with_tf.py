# -*- coding: utf-8 -*-
"""0_1NN REGRESSION WITH TF

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RWWseWh-S936_MURAG_ZtUvjlgCzcFod

# START
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


print(tf.__version__)

x=np.array([-7.0,4.0,-1.0,5,1,5,7,8])
y=np.array([3.0,6.0,9.0,12.0,0,9,8,7])
X_test=tf.constant(x)
Y_test=tf.constant(y)
plt.scatter(x,y);

"""# INPUT AND OUTPUT SHAPES"""

# create a demo tensor for
house_info = tf.constant(["bedroom","bathroom","garage"])
house_price = tf.constant([939700])
house_info, house_price

input_shape=x.shape
output_shape = y.shape
input_shape,output_shape

# another way of doing the below given programm
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))

#lets build a neural network for determination of a single digit numbers as an example
#setting random seed
tf.random.set_seed(42)

#1. creating a model using sequential API
model=tf.keras.Sequential([
    tf.keras.layers.Dense(1)
])#here the sequential command means that the model will run steps below this and including this sequentially

#2. compiling the model
model.compile(loss=tf.keras.losses.mae,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["mae"])

#3. fitting the model
model.fit(X_train_subset,Y_train_subset,epochs=3)
#epochs means the number of oppurtunities we are giving the programm to find the right and the best possible answer for the given question

X,Y

#turning the numpy arrays into tensors
X= tf.cast(tf.constant(x), dtype=tf.float32)
Y= tf.cast(tf.constant(y), dtype=tf.float32)
X,Y

# trying and making a prediction using our model
model.predict([17.0])
Y_pred = model.predict([17.0])
Y_pred + 11 =
# why do we add the mae to the y prediction ?? answers will be in

"""# Improving our model"""

# 1. create a model(specified to the problem)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(100, activation = "relu"),
    tf.keras.layers.Dense(100, activation = "relu"),
    tf.keras.layers.Dense(100, activation = "relu"),
    tf.keras.layers.Dense(1)#overfitting

])

# 2. Compile the model
model.compile(loss = tf.keras.losses.mae,
              optimizer = tf.keras.optimizers.legacy.SGD(),
              metrics = ["mae"])

# 3. fit the model
model.fit(X_train_full, Y_train_full, epochs=100)

#kets rebuild our model
#1. create the model

# there is a need of 3 datasets and there will
# training, validation, test
#

"""#VISUALIZING THE MODEL"""

model.summary()

tf.random.set_seed(42)

# 1. create a model(same as above)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, input_shape=[1], name="input_layer"),
  tf.keras.layers.Dense(1, name="output_layer")],
                            name = "model_1"
                            )

#2. compile the model
model.compile(loss = tf.keras.losses.mae,
              optimizer = tf.keras.optimizers.SGD(),
              metrics = ["mae"])

Y_test

model.evaluate(X_test, Y_test)

model.summary()

#lets create a model which build automatically by  defining the input_shape argument
tf.random.seet_seed(42)

# create a model (same as above)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_shape=[1])
])

"""Depending on the problem you are working on, there will be different evaluation your models performance

since we are working on a regression, two of the main metrics:
MAE= mean absolute error, "on average, how wrong is each of my model's predictions"
MSE -  mean squre error, "square the average error"


"""

# Evaluate the model on the test
model.evaluate(X_test, y_test)

#for calculating the mean absolute error we have one more way of programming it
mae=tf.metrics.mean_absolute_error(Y_true=t_test,y_pred=y_pred)
#for calculating mean square error
tf.keras.losses.mse
mse=tf.metrics.mean_square_error(Y_true=t_test,y_pred=y_pred)
#making functions to use mae and mse later on
def mae(Y_true, y_pred):
  return tf.metrics.mean_absolute_error(Y_true=t_test,y_pred=y_pred)
def mse(Y_true, y_pred)
return tf.metrics.mean_square_error(Y_true=t_test,y_pred=y_pred)

x=np.array([-7.0,4.0,-1.0,5,1,5,7,8])
y=np.array([3.0,6.0,9.0,12.0,0,9,8,7])
x_train = tf.constant(x)
y_train = tf.constant(y)

x_train , y_train

"""building a model requires experimentation and recurring evaluation of the model
1. getting more data - get more examples of data to make the model train better and there will be a high probability of having more accuracy

2. Make your model larger (using a more complex model

3. Train for longer - give your model more chance to find patterns in the data

there for the three requirements we need to do a lot of experiments and basically we will do 3 types of experiments

1- model-1 - same as the original mode, 1 layer, epochs = 100
2. model_2 - 2 layers, trianed for 100 epochs
3. mode;_3 - 2 layers, epochs = 500
"""

# creating model one
tf.random.set_seed(42)


model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(1)
                               ])
model_1.compile(loss = tf.keras.losses.mae ,
               optimizer = tf.keras.optimizers.SGD(),
               metrics = ["mae"])
model_1.fit(tf.expand_dims(x_train, axis=1) , y_train, epochs = 100)

y_preds_1 = model_1.predict(x_train)
def plot_predictions(train_data=x_train,
                     train_labels=y_train,
                     predictions=y_preds_1):



  plot_predictions(predictions = y_pred_1)

  #graph not produced why ??



"""# Comparing the results
comparing the results of the experiments that we have ran on our model
"""

#lets compare our models results using a pandas Data frame
import pandas as pd

model_results =[["model_1", mae_1.numpy(),mse_1.numpy()],
                ["model_2", mae_2.numpy(), mse_2.numpy()],
                ["model_3", mae_3.numpy() , mse_3.numpy()]]

all_results = pd.DataFrame(model_results, columns=["model","mae","mse"])
all_results

"""one of our main goals should be to minimize the time between our experiments. The more experiments we do, the more things youll figure out which dont work and in turn, get closer to figuring out what does work.Remember the machine learning practioners motto:"experiment, experiment, experiments".

## Tracking your experiments

* TensorBoard - a component of the tensorflow library to help track modelling experiments (we'll see this one later).
*Weights and biases - a tool for tracking all of kinds of machine learning experiments(plugs straight into tensorboard).

#Saving our models
Saving our models allows us to use them outside of google collab(or whatever they were trained )such as in a web application or a mobile app.
There are two main formats we can save our models's too:

1. the SavedModel format
2. the HDF5 format
"""

#tf.keras.model.load_model

# Save model using the SaveModel format
model_2.save("best_model_SavedModel_format")

# Save model using the HDF5 format
model_2.save("best_model_HDF5_formart.h5")

# Load in the SavedModel format model
loaded_SaveModel_format = tf.keras.models.load_model("best_model_SavedModel_format")
loaded_SavedModel_format.summary()

# Compare model_2 predictions with SavedModel format model predictions
model_2_preds = model_2.predict(X_test)
loaded_SavedModel_format_preds = loaded_SavedModel_format.predict(X_test)
model_2_preds == loaded_SavedModel_format

model_2_preds , loaded_SavedModel_format_preds

mae(Y_true = y_test , y_pred=model_2_pred) == mae(y_true=y_test , y_pred=loaded_SavedModel_format_preds)



# making a regression model using a dataset from kaggle
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# reading the data set
i = pd.read_csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")
i

i["sex"]

#we have to one hot encode the dataset
pd.get_dummies(i)

i_one_he = pd.get_dummies(i)
i_one_he.head()

# creating x and y values
x= i_one_he.drop("charges",axis =1)
y = i_one_he["charges"]
x.head()

y.head()

#creating training and test sets
#here we will the library called as scikit learn to divide the dataset into the best possible default dataset for training and test set purposes
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8 , random_state=42)
len(x),len(x_train),len(x_test)
#here we can either use train_size = n or we can also use test_size= n

#building a neural network
tf.random.set_seed(42)

#1.
i_model = tf.keras.Sequential([tf.keras.layers.Dense(500),
                               tf.keras.layers.Dense(500),
                               tf.keras.layers.Dense(500),
                             tf.keras.layers.Dense(50)
                             ])
i_model.compile(loss = tf.keras.losses.mae,
                optimizer = tf.keras.optimizers.Adam(),
                metrics=["mae"])

history = i_model.fit(x_train,y_train,epochs = 200, verbose =1)

i_model.evaluate(x_test , y_test)

#lets plot history(also known as the loss curve or a training curve)

pd.DataFrame(history.history).plot()

# lets increase the complexity of the model
#here we will add one more layer of hidden network to let the model learn more

# learning about early stopping call back

"""# Preprocessing data (normalization and standardization)"""

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler , OneHotEncoder

#create a column transformer
ct = make_column_transformer(
    (MinMaxScaler(), ["age","bmi","children"]),
    (OneHotEncoder(handle_unknown = "ignore"),["sex","smoker","region"])
)

#create x and y
x = i.drop("charges",axis = 1)
y = i["charges"]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)

ct.fit(x_train)

x_train_normal = ct.transform(x_train)
x_test_normal = ct.transform(x_test)

x_train_normal = ct.transform(x_train)
x_test_normal = ct.transform(x_test)

#here we used the normalization(MinMaxScaler)

x_train_normal = pd.DataFrame(x_train_normal)
x_train_normal[0]

x_train.shape, x_train_normal.shape

!pip install sklearn.compose

#after this make the same neural as before and but in space of x_train we will put x_trai_normal

