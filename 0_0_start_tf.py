# -*- coding: utf-8 -*-
"""0.0 start tf

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11UZTAypXuBiZc7mfsLZinlI7t703A74z
"""

#In this notebook we are going to focus on some of the fundamentals of the tensor flow libraries

"""we are going to cover about


*   Lintroduction of numpy

getting info from tensors.
manipulating tensors.
tensors and numpy
using @tf.function(a way to speed up your regular python function.
using GPUs with TensorFlow(or TPUs)
Exercise to try for yourself.



"""

# Import TensorFlow
import tensorflow as tf
print(tf.__version__)

#Create tensors with tf.constant()
scalar=tf.constant(7)
scalar

# check the dimension of our vector
vector = tf.constant([10,10])
vector
vector.ndim

vector

#create a matrix (has more than 1 dimesnison)
matrix = tf.constant([[7,10],[10,10]])
matrix
matrix.ndim

# Create another matrix
matrix2 = tf.constant([[7.,7.],[10.,10.],[5.,5.]], dtype=tf.float16) #specifying the data type with dtype parameter

matrix2
matrix2.ndim

matrix2

#lets create a tensor
tensor=tf.constant([[[1,2,3,3],[4,5,6,7]],[[7,8,9,10],[10,11,12,1]],[[10,10,10,10],[1,1,1,1]]])
tensor

tensor.ndim

#we have created so far
#scalar: a single number
#matrix: a 2 dimensional array of numbers
#tensor: as n dimensional array of numbers(when n can be any number, a 0-dimensional tensor, a 1-dimensional tensor is a vector)

tf.Variable

# creating a tensor with tf.Variable() as above
changeable_tensor = tf.Variable([10,7])
unchangeable_tensor = tf.constant([10,8])
changeable_tensor, unchangeable_tensor

#lets try to change one of the element
changeable_tensor[0].assign(7)
changeable_tensor

#creating random tensors
# random tensors are tensors of some of the arbitrary siz whwich contain random numbers.
# input-numerical coding-learns representation-representation outputs-outputs

#creating 2 random but the same tensors
random_1 = tf.random.Generator.from_seed(42)#set seed for reproducibility
random_1 = random_1.normal(shape=(3,2))
random_2 = tf.random.Generator.from_seed(42)
random_2 = random_2.normal(shape=(3,2))

#checking their equality
random_1,random_2,random_1==random_2

#shuffle the order of the elements in tensor
#shuffling a tensor is required when you want to shuffle your data so the inherent order doesnt effect learning
not_shuffled = tf.constant([[10,7],[3,4],[2,5]])
#shuffle our non-shuffled tensor
tf.random.shuffle(not_shuffled)

not_shuffled
#shuffle our non-shuffled tensor
tf.random.set_seed(42)
tf.random.shuffle(not_shuffled, seed=42)

not_shuffled

tf.random.set_seed(42)#global level random seed
tf.random.shuffle(not_shuffled, seed=42) # operation level random seed

# OTHER WAYS TO MAKE TENSOR
tf.ones([10,7])

# Create a tensor all zeroes
tf.zeros(shape=(3,4))

"""Turning Nimpy array into tensors

The main difference between NumPy arrays and tensoflow tensors is that temnors can be run on a GPU(much faster for numerical computing).
"""

#turning array in NUmpy into tensors flow
import numpy as np
numpy_b = np.arange(1,22)
numpy_b
numpy_A = np.arange(1, 25, dtype=np.int32)
numpy_A

# X = tf.constant(some_matrix) # capital for matrix or sensor
# Y = tf.constant(vector) # non-capital for vector

a= tf.constant(numpy_A, shape=(2,3,4))
b= tf.constant(numpy_A)
a,b

2*3*4

A = tf.constant(numpy_A, shape =(3,8))
B = tf.constant(numpy_A)
A,B

# shape- the length(number of elements) of each of the dimension of a tensor.--- tensor.shape
# Rank- the number of tensor dimensions, A scalar has rank 0 , a vector has rank q, a matrix is rank 2, a tensor has rank n--- tensor.ndim
# Axis or dimension  a particular dimension of a tensor---- tensor[0],tensor[:,1]
# for size of the matrix ---- tf.size(tensor)

# create a rank 4 tensor(4 dimesnisons)
rank_4_tensor = tf.zeros(shape=[2,3,4,5])
rank_4_tensor

rank_4_tensor.shape, rank_4_tensor.ndim , tf.size(rank_4_tensor)

# get variosu attributes of our tensor
print("Datatype of every element:",rank_4_tensor.dtype)
print("number of dimensions(rank):", rank_4_tensor.ndim)
print("shape of the tensor:", rank_4_tensor.shape)
print("elements along the 0 axis:", rank_4_tensor.shape[0])
print("elements along the last axis:", rank_4_tensor.shape[-1])
print("Total number of elements in our tensor:", tf.size(rank_4_tensor).numpy())

#indexing tensors
#tensors can be indexed just like python











#manipulating tensors
# we can add values to a tensors using addition operator
tensor = tf.constant([[1,2],[2,3]])
tensor+10
# this is a case of temporary addition of 10 an integer to the tensors where the vale of the parent tensors is still the same and it stay unchanges untill we give command such as \
tensor=tensor+10
tensor

# multiplication
tensor*10
# same will be with subtraction and division

# we can use inbuilt functins in tensorflow for multiply and different arithmetic operation as well
tf.math.multiply(tensor,10)

"""MATRIX MULTIPLICATION
In machine learning, matrix multiplication is one of the most common tensor operation.
"""

tf.print(tensor)
tf.matmul(tensor,tensor)
#here the value is tensor times tensor but when we write it in a different way then the answer will be different

tensor*tensor

tensor,tensor

X = tf.constant([[3,2],[2,2],[4,5]])
Y = tf.constant([[1,2],[2,5],[8,9]])
tf.matmul(X,Y)

# rules for matrix multiplication
# the inner dimension must match
#the resulting must havve the safe of the inner dimension the matrix
# so if we take 2 matrix of 3 rows and 2 columns then we need decrease or squeeze the matrix to make it a square matrix

#now lets reshape y matrix
tf.reshape(Y,shape=(2,3))

tf.print(Y)

tf.matmul(X,Y)

tf.matmul(X,tf.reshape(Y,shape=(2,3)))

tf.matmul(tf.reshape(X,shape=(2,3)),Y)

tf.matmul(tf.transpose(X),Y)

tf.transpose(X)

# matrix multiplication is also reffered to as dot product
#tf.matmul()
#tf.tensordot()

#perform the dot product on X and Y (requires X or Y to be transposed)
tf.tensordot(tf.transpose(X),Y,axes=1)

# perform matrix multiplication using y transpose
tf.matmul(X,tf.transpose(Y))

tf.matmul(X,tf.reshape(Y,shape=(2,3)))

"""Both the above multiplication are ideologically same but they provide different answer because of the way the computer processes the give se of the matrix and there is a difference between transpose and reshaping the matrix as due to which both the matrix produce different answers.

So if in a case where the the matrix multiplication does not happen because of the mismatch of matrix then we will always transpose and we will avoid reshaping  it as it might give us the wrong answer but would not show any error when calculation but it will definitely will be a logical error. and on transposing the matrix multiplication also abides to the rules of matrix multiplication

# CHANGING THE DATATYPE OF A TENSOR
"""

#create a new tensor with default datatype(float 32)
B = tf.constant([1.7,7.4])
B.dtype

c = tf.constant([7,10])
c.dtype

B = tf.cast(B,dtype = tf.float16)
B,B.dtype

"""# Aggregating tensors

aggregating tensors = condensing them from multiple values down to a smaller amount of values
"""

x = tf.constant([-7,-10])
tf.abs(x)

"""Lets go through the following forms of aggregation:
get minimum
get maximum
get the mean of a tensor
get the sum of a tensor
"""

e = tf.constant(np.random.randint(0,100, size = 50))
e

tf.size(e), e.shape, e.ndim

tf.reduce_min(e)
#tf.reduce_max(e)
#tf.reduce_mean(e)
#tf.reduce_sum(e)
#tf.reduce_varience(e)

"""# POSITIONAL MIN AND MAX

"""

import tensorflow_probability as tfp
tfp.stats.variance(e)

tf.random.set_seed(42)
f=tf.random.uniform(shape=[50])
f

tf.argmax(f)
#it finds the index of the maximum position

np.argmax(f)
#it finds the index of maximum position

# Index on our largest value position
f[tf.argmax(f)]
# or we can use tf.reduce_max()
#these 2 syntaxes find the max value in that found position

#check for equality
f[(tf.argmax(f))] == tf.reduce_max(f)

"""#squeezeing a tensor"""

tf.random.set_seed(42)
g=tf.random.uniform(shape=[1,1,1,50])
g

g.shape

g_squeezed = tf.squeeze(g)
g_squeezed

g_squeeze=g
assert g_squeezed=g

"""# One Hot Encoding



"""

list1 = [0,1,2,3]
tf.one_hot(list1,depth=4)
tf.one_hot(list1,depth=4, on_value="husky" ,off_value="meow")

"""# Integration of GPU for calculating complex equations"""

tf.config.list_physical_devices()

import tensorflow as tf
tf.config.list_physical_devices("GPU")

!nvidia-smi

#THE END